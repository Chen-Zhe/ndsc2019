{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import file_path as fp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import collections\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(fp.csv_folder+\"mobile_data_info_val_competition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add image features to validation data\n",
    "resnet_features = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', pooling=\"avg\")\n",
    "\n",
    "def get_resnet_features(image_path):\n",
    "  try:\n",
    "    im = cv2.resize(cv2.imread(fp.image_base_folder+image_path), (224, 224)).astype(np.float32)\n",
    "    # standardization: remove mean of ISLVRC2012 dataset\n",
    "    im[:,:,0] -= 103.939\n",
    "    im[:,:,1] -= 116.779\n",
    "    im[:,:,2] -= 123.68\n",
    "    # Insert a new dimension for the batch_size\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    return resnet_features.predict(im)[0]\n",
    "  except:\n",
    "    return None\n",
    "\n",
    "validation_data[\"image_vector\"] = validation_data.apply(lambda x: get_resnet_features(x.image_path), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.to_pickle(fp.csv_folder+\"mobile_validation_with_resnet50_vector_and_word2vec.pickle\")\n",
    "input_data.to_pickle(fp.csv_folder+\"mobile_training_with_resnet50_vector_and_word2vec.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_pickle(fp.csv_folder+\"mobile_training_with_resnet50_vector_and_word2vec.pickle\")\n",
    "validation_data = pd.read_pickle(fp.csv_folder+\"mobile_validation_with_resnet50_vector_and_word2vec.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_phone_number(string):\n",
    "  if len(string) <= 3:\n",
    "    return False\n",
    "  digit_count = 0\n",
    "  for char in string:\n",
    "    if char.isdigit():\n",
    "      digit_count += 1\n",
    "      \n",
    "  if digit_count > 3:\n",
    "    return True\n",
    "  \n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 1653320\n"
     ]
    }
   ],
   "source": [
    "vocab_sentence = list()\n",
    "sentence_id = 0\n",
    "\n",
    "for line in np.concatenate([input_data.title.values, validation_data.title.values]):\n",
    "  tokens = line.split()\n",
    "  for token in tokens:\n",
    "    # remove telephone numbers\n",
    "    if len(token) == 1 and not token.isdigit():\n",
    "      continue\n",
    "    if is_phone_number(token):\n",
    "      continue\n",
    "    if token == \"whatsapp\" or token == \"wa\":\n",
    "      continue\n",
    "      \n",
    "    vocab_sentence.append((token, sentence_id))\n",
    "  \n",
    "  sentence_id += 1\n",
    "\n",
    "vocabulary, sentence_id_map = list(zip(*vocab_sentence))\n",
    "  \n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words [('samsung', 41458), ('promo', 41155), ('garansi', 38850), ('iphone', 34248), ('1', 31277)]\n",
      "Sample data [54, 4, 239, 240, 124, 0, 0, 13, 0, 134] ['apple', 'iphone', '4s', 'back', 'glass', 'UNK', 'UNK', 'original', 'UNK', 'putih']\n",
      "length of the dictionary:  400 should be equal to 400\n",
      "least common words [('keren', 466), ('to', 466), ('7plus', 461), ('touch', 449), ('cell', 446)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "vocabulary_size = 400\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "  \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    index = dictionary.get(word, 0)\n",
    "    if index == 0:  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reversed_dictionary\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)\n",
    "# del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words', count[1:6])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "print(\"length of the dictionary: \", len(reverse_dictionary), \"should be equal to\", vocabulary_size)\n",
    "print(\"least common words\", count[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Term Frequency - Inverse Term Frequency for most common words\n",
    "known_word_set = set([word for word, _ in count[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple iphone 4s back glass spare part original replacement putih'\n",
      " 'iphone 4s 64gb white']\n",
      "239 4s -> 54 apple\n",
      "239 4s -> 4 iphone\n",
      "240 back -> 124 glass\n",
      "240 back -> 239 4s\n",
      "124 glass -> 0 UNK\n",
      "124 glass -> 0 UNK\n",
      "0 UNK -> 124 glass\n",
      "0 UNK -> 0 UNK\n",
      "0 UNK -> 13 original\n",
      "0 UNK -> 0 UNK\n",
      "13 original -> 0 UNK\n",
      "13 original -> 0 UNK\n",
      "0 UNK -> 0 UNK\n",
      "0 UNK -> 134 putih\n",
      "134 putih -> 0 UNK\n",
      "134 putih -> 13 original\n",
      "4 iphone -> 11 64gb\n",
      "4 iphone -> 239 4s\n",
      "239 4s -> 4 iphone\n",
      "239 4s -> 11 64gb\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "# new\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "  if data_index + span > len(data):\n",
    "      data_index = 0\n",
    "  buffer.extend(data[data_index:data_index + span])\n",
    "  data_index += span\n",
    "  for i in range(batch_size // num_skips):\n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    words_to_use = random.sample(context_words, num_skips)\n",
    "    for j, context_word in enumerate(words_to_use):\n",
    "      if sentence_id_map[data_index-span+skip_window] == sentence_id_map[data_index-span+context_word]:\n",
    "        batch[i * num_skips + j] = buffer[skip_window]\n",
    "        labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "      else:\n",
    "        batch[i * num_skips + j] = 0\n",
    "        labels[i * num_skips + j, 0] = 0\n",
    "    if data_index == len(data):\n",
    "      buffer.extend(data[0:span])\n",
    "      data_index = span\n",
    "    else:\n",
    "      buffer.append(data[data_index])\n",
    "      data_index += 1\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=20, num_skips=2, skip_window=2)\n",
    "print(input_data.title.head(2).values)\n",
    "for i in range(20):\n",
    "  print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 64  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  with tf.name_scope('inputs'):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Look up embeddings for inputs.\n",
    "  with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# Construct the variables for the NCE loss\n",
    "  with tf.name_scope('weights'):\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))\n",
    "    )\n",
    "  with tf.name_scope('biases'):\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  # Explanation of the meaning of NCE loss:\n",
    "  #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "  with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weights,\n",
    "            biases=nce_biases,\n",
    "            labels=train_labels,\n",
    "            inputs=embed,\n",
    "            num_sampled=num_sampled,\n",
    "            num_classes=vocabulary_size))\n",
    "\n",
    "  # Add the loss value as a scalar to summary.\n",
    "  tf.summary.scalar('loss', loss)\n",
    "\n",
    "  # Construct the SGD optimizer\n",
    "  with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                            valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Merge all summaries.\n",
    "  merged = tf.summary.merge_all()\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "  # Create a saver.\n",
    "#   saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps to run: 258340\n",
      "Initialized\n",
      "Average loss at step 0 : 106.20327758789062\n",
      "Average loss at step 2000 : 43.026103110313414\n",
      "Average loss at step 4000 : 13.900485816717147\n",
      "Average loss at step 6000 : 8.633106861352921\n",
      "Average loss at step 8000 : 6.6948703126907345\n",
      "Average loss at step 10000 : 5.673398793458938\n",
      "Average loss at step 12000 : 5.1663813798427585\n",
      "Average loss at step 14000 : 4.819228715777397\n",
      "Average loss at step 16000 : 4.571663630127907\n",
      "Average loss at step 18000 : 4.391167924284935\n",
      "Average loss at step 20000 : 4.281222813487053\n",
      "Average loss at step 22000 : 4.188865005254746\n",
      "Average loss at step 24000 : 4.113173606157303\n",
      "Average loss at step 26000 : 4.070345463991165\n",
      "Average loss at step 28000 : 4.005630286693573\n",
      "Average loss at step 30000 : 3.965192820906639\n",
      "Average loss at step 32000 : 3.9357951723337172\n",
      "Average loss at step 34000 : 3.9155394170284272\n",
      "Average loss at step 36000 : 3.8770740791559217\n",
      "Average loss at step 38000 : 3.859020723104477\n",
      "Average loss at step 40000 : 3.8337826390266416\n",
      "Average loss at step 42000 : 3.816112325310707\n",
      "Average loss at step 44000 : 3.7909468178749086\n",
      "Average loss at step 46000 : 3.7797700034379957\n",
      "Average loss at step 48000 : 3.7613393379449844\n",
      "Average loss at step 50000 : 3.7544066541194914\n",
      "Average loss at step 52000 : 3.753038062095642\n",
      "Average loss at step 54000 : 3.7335169736146927\n",
      "Average loss at step 56000 : 3.715286073923111\n",
      "Average loss at step 58000 : 3.7149715739488602\n",
      "Average loss at step 60000 : 3.712479004740715\n",
      "Average loss at step 62000 : 3.691907282471657\n",
      "Average loss at step 64000 : 3.685174875855446\n",
      "Average loss at step 66000 : 3.677384186387062\n",
      "Average loss at step 68000 : 3.664762677550316\n",
      "Average loss at step 70000 : 3.6586576282978056\n",
      "Average loss at step 72000 : 3.6482445272207262\n",
      "Average loss at step 74000 : 3.640500602722168\n",
      "Average loss at step 76000 : 3.6392260645627976\n",
      "Average loss at step 78000 : 3.6469491624832155\n",
      "Average loss at step 80000 : 3.624444211244583\n",
      "Average loss at step 82000 : 3.6185838438272477\n",
      "Average loss at step 84000 : 3.622398115515709\n",
      "Average loss at step 86000 : 3.6207013468742373\n",
      "Average loss at step 88000 : 3.6040909210443495\n",
      "Average loss at step 90000 : 3.6072063353061674\n",
      "Average loss at step 92000 : 3.594876186609268\n",
      "Average loss at step 94000 : 3.5901486563682554\n",
      "Average loss at step 96000 : 3.585222242832184\n",
      "Average loss at step 98000 : 3.5794557770490645\n",
      "Average loss at step 100000 : 3.5692817310094833\n",
      "Average loss at step 102000 : 3.5776136113405226\n",
      "Average loss at step 104000 : 3.5773176136016844\n",
      "Average loss at step 106000 : 3.563720371723175\n",
      "Average loss at step 108000 : 3.55745521903038\n",
      "Average loss at step 110000 : 3.5672717138528824\n",
      "Average loss at step 112000 : 3.5606390733718873\n",
      "Average loss at step 114000 : 3.5516705605983736\n",
      "Average loss at step 116000 : 3.5535093562602995\n",
      "Average loss at step 118000 : 3.5444736322164534\n",
      "Average loss at step 120000 : 3.5442327938079834\n",
      "Average loss at step 122000 : 3.5335999633073807\n",
      "Average loss at step 124000 : 3.529928059697151\n",
      "Average loss at step 126000 : 3.527429869771004\n",
      "Average loss at step 128000 : 3.5338776963949203\n",
      "Average loss at step 130000 : 3.5264886984825132\n",
      "Average loss at step 132000 : 3.528060612678528\n",
      "Average loss at step 134000 : 3.5129986486434936\n",
      "Average loss at step 136000 : 3.5280474882125854\n",
      "Average loss at step 138000 : 3.5177179520130157\n",
      "Average loss at step 140000 : 3.51522722864151\n",
      "Average loss at step 142000 : 3.5114084055423738\n",
      "Average loss at step 144000 : 3.5064539889097213\n",
      "Average loss at step 146000 : 3.5086460970640183\n",
      "Average loss at step 148000 : 3.497232671260834\n",
      "Average loss at step 150000 : 3.493086727976799\n",
      "Average loss at step 152000 : 3.495687078475952\n",
      "Average loss at step 154000 : 3.5002251739501955\n",
      "Average loss at step 156000 : 3.494781386613846\n",
      "Average loss at step 158000 : 3.4946312540769577\n",
      "Average loss at step 160000 : 3.4874695090055465\n",
      "Average loss at step 162000 : 3.495861568212509\n",
      "Average loss at step 164000 : 3.487570012331009\n",
      "Average loss at step 166000 : 3.4851650346517564\n",
      "Average loss at step 168000 : 3.4841289346218107\n",
      "Average loss at step 170000 : 3.4772882363796236\n",
      "Average loss at step 172000 : 3.4822094180583956\n",
      "Average loss at step 174000 : 3.470769119858742\n",
      "Average loss at step 176000 : 3.468135541200638\n",
      "Average loss at step 178000 : 3.4675117485523224\n",
      "Average loss at step 180000 : 3.4727889337539675\n",
      "Average loss at step 182000 : 3.4721678265333176\n",
      "Average loss at step 184000 : 3.469161656737328\n",
      "Average loss at step 186000 : 3.4633185596466065\n",
      "Average loss at step 188000 : 3.4698542207479477\n",
      "Average loss at step 190000 : 3.46515169942379\n",
      "Average loss at step 192000 : 3.4604146645069123\n",
      "Average loss at step 194000 : 3.463672459244728\n",
      "Average loss at step 196000 : 3.452166676402092\n",
      "Average loss at step 198000 : 3.4584715386629106\n",
      "Average loss at step 200000 : 3.449970007777214\n",
      "Average loss at step 202000 : 3.4471444883346556\n",
      "Average loss at step 204000 : 3.4449350991249084\n",
      "Average loss at step 206000 : 3.4570595791339875\n",
      "Average loss at step 208000 : 3.448161472082138\n",
      "Average loss at step 210000 : 3.4501140419244765\n",
      "Average loss at step 212000 : 3.4399293146133423\n",
      "Average loss at step 214000 : 3.453204114317894\n",
      "Average loss at step 216000 : 3.4450034182071687\n",
      "Average loss at step 218000 : 3.4419275290966036\n",
      "Average loss at step 220000 : 3.444007712006569\n",
      "Average loss at step 222000 : 3.4328697551488876\n",
      "Average loss at step 224000 : 3.4403724892139436\n",
      "Average loss at step 226000 : 3.4332796528339387\n",
      "Average loss at step 228000 : 3.429560182213783\n",
      "Average loss at step 230000 : 3.427920085191727\n",
      "Average loss at step 232000 : 3.4387770377397535\n",
      "Average loss at step 234000 : 3.4314072618484497\n",
      "Average loss at step 236000 : 3.4356293275356293\n",
      "Average loss at step 238000 : 3.4231775475740434\n",
      "Average loss at step 240000 : 3.4390091474056246\n",
      "Average loss at step 242000 : 3.424553339600563\n",
      "Average loss at step 244000 : 3.43121948325634\n",
      "Average loss at step 246000 : 3.424442007422447\n",
      "Average loss at step 248000 : 3.4188174378871916\n",
      "Average loss at step 250000 : 3.4240100713968276\n",
      "Average loss at step 252000 : 3.423687325000763\n",
      "Average loss at step 254000 : 3.4125611107349396\n",
      "Average loss at step 256000 : 3.4138270943164826\n",
      "Average loss at step 258000 : 3.424085507154465\n",
      "Nearest to termurah: sein, khusus, iphone, mito, gudang, oneplus, ini, 105\n",
      "Nearest to second: lengkap, back, lite, 64, f9, single, matte, orginal\n",
      "Nearest to ini: khusus, tpu, z5, stock, s8, lewat, seru, dijual\n",
      "Nearest to sim: carbon, terbatas, dapat, gb, buy, laptop, ready, mix\n",
      "Nearest to baru: boskuu, orginal, a3s, laptop, harga, ready, infinix, set\n",
      "Nearest to cuci: serius, diskon, bnib, s8, pemesanan, blue, serba, silahkan\n",
      "Nearest to zenfone: ongkir, spesial, kabel, charger, gores, to, glass, gray\n",
      "Nearest to bnib: cuci, s8, new, 32, space, a2, j7, info\n",
      "Nearest to 64: 128gb, pc, second, gb, mobile, cable, ze620kl, z5\n",
      "Nearest to 3: 3gb, smart, data, 5, 6gb, lcd, camera, internal\n",
      "Nearest to diskon: minus, cuci, vibe, z3, series, serba, global, asli\n",
      "Nearest to lenovo: samsung, leather, to, m1, original, vandroid, no, 5c\n",
      "Nearest to grey: belakang, silver, vivo, docomo, gold, gray, mix, 6s\n",
      "Nearest to dual: internal, flash, ipad, advan, camera, aksesoris, single, 6\n",
      "Nearest to UNK: grs, double, buy, core, lite, gold, lcd, s5\n",
      "Nearest to asus: shopee, wireless, beli, cuci, vibe, berkualitas, xiomi, redmi\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "data_index = 0\n",
    "\n",
    "num_epochs = 10\n",
    "num_steps = (len(data) * num_skips // batch_size + 1) * num_epochs\n",
    "print(\"Steps to run:\", num_steps)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "#   Open a writer to write summaries.\n",
    "#   writer = tf.summary.FileWriter(fp.log_dir, session.graph)\n",
    "\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # Define metadata variable.\n",
    "    run_metadata = tf.RunMetadata()\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "    # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "    _, summary, loss_val = session.run(\n",
    "        [optimizer, merged, loss],\n",
    "        feed_dict=feed_dict,\n",
    "        run_metadata=run_metadata)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    # Add returned summaries to writer in each step.\n",
    "#     writer.add_summary(summary, step)\n",
    "    # Add metadata to visualize the graph for the last run.\n",
    "#     if step == (num_steps - 1):\n",
    "#       writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step', step, ':', average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "#     if step % 10000 == 0:\n",
    "#       sim = similarity.eval()\n",
    "#       for i in range(valid_size):\n",
    "#         valid_word = reverse_dictionary[valid_examples[i]]\n",
    "#         top_k = 8  # number of nearest neighbors\n",
    "#         nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "#         log_str = 'Nearest to %s: %s' % (valid_word, \", \".join([reverse_dictionary[k] for k in nearest]))\n",
    "#         print(log_str)\n",
    "  \n",
    "  sim = similarity.eval()\n",
    "  for i in range(valid_size):\n",
    "    valid_word = reverse_dictionary[valid_examples[i]]\n",
    "    top_k = 8  # number of nearest neighbors\n",
    "    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "    log_str = 'Nearest to %s: %s' % (valid_word, \", \".join([reverse_dictionary[k] for k in nearest]))\n",
    "    print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "  # Write corresponding labels for the embeddings.\n",
    "#   with open(fp.log_dir + '/metadata.tsv', 'w') as f:\n",
    "#     for i in range(vocabulary_size):\n",
    "#       f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "  # Save the model for checkpoints.\n",
    "#   saver.save(session, os.path.join(fp.log_dir, 'model.ckpt'))\n",
    "\n",
    "  # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "#   config = projector.ProjectorConfig()\n",
    "#   embedding_conf = config.embeddings.add()\n",
    "#   embedding_conf.tensor_name = embeddings.name\n",
    "#   embedding_conf.metadata_path = os.path.join(fp.log_dir, 'metadata.tsv')\n",
    "#   projector.visualize_embeddings(writer, config)\n",
    "\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_title_vector(itemid, title):\n",
    "  tokens = title.split()\n",
    "  total_score = 0.0\n",
    "  total_vec = np.zeros(embedding_size)\n",
    "  for token in tokens:\n",
    "    if token not in known_word_set:\n",
    "      continue\n",
    "    word_vec = final_embeddings[dictionary[token]]\n",
    "    tf_idf_score = 1.0\n",
    "    total_score += tf_idf_score\n",
    "    total_vec += word_vec * tf_idf_score\n",
    "  \n",
    "  if total_score == 0.0:\n",
    "    return total_vec\n",
    "  return total_vec / total_score\n",
    "\n",
    "input_data[\"title_vector\"] = input_data.apply(lambda x: get_avg_title_vector(x.itemid, x.title), axis=1)\n",
    "validation_data[\"title_vector\"] = validation_data.apply(lambda x: get_avg_title_vector(x.itemid, x.title), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(guesses, correct, k):\n",
    "  for i in range(k):\n",
    "    if guesses[i] == correct:\n",
    "      return 1 / (i + 1), i + 1\n",
    "  return 0.0, 0\n",
    "\n",
    "def proba_to_guesses(probs, classes, k):\n",
    "  ret = list()\n",
    "  \n",
    "  for idx in np.argsort(probs)[-k:]:\n",
    "    ret.append(classes[idx])\n",
    "  \n",
    "  ret.reverse()\n",
    "  return ret\n",
    "\n",
    "def score_model(proba_vector, ground_truth, classes, k):\n",
    "  assert proba_vector.shape[1] == len(classes)\n",
    "  assert proba_vector.shape[0] == ground_truth.shape[0]\n",
    "  total_score = 0.0\n",
    "  \n",
    "  stat = [0] * (k + 1)\n",
    "  \n",
    "  for item_vec, correct in zip(proba_vector, ground_truth):\n",
    "    result, idx = map_at_k(proba_to_guesses(item_vec, classes, k), correct, k)\n",
    "    total_score += result\n",
    "    stat[idx] += 1\n",
    "  return total_score / proba_vector.shape[0], stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=64, random_state=12345)\n",
    "\n",
    "reduced_image_vector = pca.fit_transform(np.stack(input_data.image_vector.values))\n",
    "\n",
    "input_data[\"image_vector_64\"] = np.vsplit(reduced_image_vector, indices_or_sections=reduced_image_vector.shape[0])\n",
    "\n",
    "validation_reduced_image_vector = pca.transform(np.stack(validation_data.image_vector.values))\n",
    "\n",
    "validation_data[\"image_vector_64\"] = np.vsplit(validation_reduced_image_vector,\n",
    "                                               indices_or_sections=validation_reduced_image_vector.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System (0.9540636042402827, [130, 8354, 572])\n",
      "Features (0.76775956284153, [1110, 5324, 1435])\n",
      "Network Connections (0.903735325506937, [137, 3920, 628])\n"
     ]
    }
   ],
   "source": [
    "#dry run\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for y_col in input_data.columns.values[3:6]:\n",
    "  train, test = train_test_split(input_data[input_data[y_col] != -1], test_size=0.2, random_state=12345)\n",
    "  \n",
    "  if train[y_col].unique().shape[0] > 100:\n",
    "    significant = set(train[[y_col, \"itemid\"]].groupby(y_col).agg(\"count\")\\\n",
    "      .sort_values(\"itemid\", ascending=False).head(100).index.values)\n",
    "    train = train[train.apply(lambda x: x[y_col] in significant, axis=1)]\n",
    "    \n",
    "  clf = RandomForestClassifier(n_estimators=200, random_state=0, n_jobs=-1)\n",
    "  clf.fit(np.concatenate(\n",
    "    (np.stack(train.title_vector.values), np.stack(train.image_vector_64.values).reshape(-1, 64))\n",
    "    , axis=1), train[y_col])\n",
    "  \n",
    "  print(y_col, score_model(\n",
    "    clf.predict_proba(\n",
    "        np.concatenate(\n",
    "        (np.stack(test.title_vector.values), np.stack(test.image_vector_64.values).reshape(-1, 64))\n",
    "        , axis=1)\n",
    "      )\n",
    "      , test[y_col].values, clf.classes_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System (0.9686947879858657, [95, 8584, 377])\n",
      "Features (0.7744948532215021, [999, 5319, 1551])\n",
      "Network Connections (0.9211312700106724, [91, 4037, 557])\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for y_col in input_data.columns.values[11:12]:\n",
    "  train, test = train_test_split(input_data[input_data[y_col] != -1], test_size=0.2, random_state=12345)\n",
    "  \n",
    "  if train[y_col].unique().shape[0] > 100:\n",
    "    significant = set(train[[y_col, \"itemid\"]].groupby(y_col).agg(\"count\")\\\n",
    "      .sort_values(\"itemid\", ascending=False).head(100).index.values)\n",
    "    train = train[train.apply(lambda x: x[y_col] in significant, axis=1)]\n",
    "  \n",
    "  clf = xgb.XGBClassifier(n_estimators=200, random_state=0, n_jobs=6, max_depth=5)\n",
    "  clf.fit(np.concatenate(\n",
    "    (np.stack(train.title_vector.values), np.stack(train.image_vector_64.values).reshape(-1, 64))\n",
    "    , axis=1), train[y_col])\n",
    "  \n",
    "  print(y_col, score_model(\n",
    "    clf.predict_proba(\n",
    "        np.concatenate(\n",
    "        (np.stack(test.title_vector.values), np.stack(test.image_vector_64.values).reshape(-1, 64))\n",
    "        , axis=1)\n",
    "      )\n",
    "      , test[y_col].values, clf.classes_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System training model...write to file...\n",
      "Features training model...write to file...\n",
      "Network Connections training model...write to file...\n",
      "Memory RAM training model...write to file...\n",
      "Brand training model...write to file...\n",
      "Warranty Period training model...write to file...\n",
      "Storage Capacity training model...write to file...\n",
      "Color Family training model...write to file...\n",
      "Phone Model training model...write to file...\n",
      "Camera training model...write to file...\n",
      "Phone Screen Size training model...write to file...\n"
     ]
    }
   ],
   "source": [
    "# actual run\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(fp.csv_folder+\"submission/mobile_no_header.csv\", \"w\") as sub_file:\n",
    "  for y_col in input_data.columns.values[3:]:\n",
    "    try:\n",
    "      train, test = input_data[input_data[y_col] != -1], validation_data\n",
    "    except:\n",
    "      # there are vector feature fields after y columns\n",
    "      break\n",
    "\n",
    "    if train[y_col].unique().shape[0] > 100:\n",
    "      significant = set(train[[y_col, \"itemid\"]].groupby(y_col).agg(\"count\")\\\n",
    "        .sort_values(\"itemid\", ascending=False).head(100).index.values)\n",
    "      train = train[train.apply(lambda x: x[y_col] in significant, axis=1)]\n",
    "    \n",
    "    print(y_col, \": training model...\", end=\"\")\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=0, n_jobs=-1)\n",
    "    clf.fit(np.concatenate(\n",
    "      (np.stack(train.title_vector.values), np.stack(train.image_vector_64.values).reshape(-1, 64))\n",
    "      , axis=1), train[y_col])\n",
    "\n",
    "    proba_vector = clf.predict_proba(\n",
    "          np.concatenate(\n",
    "          (np.stack(test.title_vector.values), np.stack(test.image_vector_64.values).reshape(-1, 64))\n",
    "          , axis=1)\n",
    "        )\n",
    "    assert validation_data.itemid.values.shape[0] == proba_vector.shape[0]\n",
    "    \n",
    "    print(\"write to file...\")\n",
    "\n",
    "    for itemid, item_vec in zip(validation_data.itemid.values, proba_vector):\n",
    "      guesses = proba_to_guesses(item_vec, clf.classes_, 2)\n",
    "      sub_file.write(f\"{itemid}_{y_col},{' '.join([str(g) for g in guesses])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
